{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sedona SpatialBench","text":"<p>Sedona SpatialBench makes it easy to run spatial benchmarks on a realistic dataset with any query engine.</p> <p>The methodology is unbiased and the benchmarks in any environment to compare relative performance between runtimes.</p>"},{"location":"#why-spatialbench","title":"Why SpatialBench","text":"<p>SpatialBench is a geospatial benchmark for testing and optimizing spatial analytical query performance in database systems. Inspired by the SSB and NYC taxi data, it combines realistic urban mobility scenarios with a star schema extended with spatial attributes like pickup/dropoff points, zones, and building footprints.</p> <p>This design enables evaluation of the following geospatial operations:</p> <ul> <li>spatial joins</li> <li>distance queries</li> <li>aggregations</li> <li>point-in-polygon analysis</li> </ul> <p>Let\u2019s dive into the advantages of SpatialBench.</p>"},{"location":"#key-advantages","title":"Key advantages","text":"<ul> <li>Uses spatial datasets with geometry columns.</li> <li>Includes queries with different spatial predicates.</li> <li>Easily reproducible results.</li> <li>Includes a dataset generator to so results are reproducible.</li> <li>The scale factors of the datasets can be changed so that you can run the queries locally, in a data warehouse, or on a large cluster in the cloud.</li> <li>All the specifications used to run the benchmarks are documented, and the methodology is unbiased.</li> <li>The code is open source, allowing the community to provide feedback and keep the benchmarks up-to-date and reliable over time.</li> </ul>"},{"location":"#generate-synthetic-data","title":"Generate synthetic data","text":"<p>Here\u2019s how you can install the synthetic data generator:</p> <pre><code>cargo install --path ./spatialbench-cli\n</code></pre> <p>Here\u2019s how you can generate the synthetic dataset:</p> <pre><code>spatialbench-cli -s 1 --format=parquet\n</code></pre> <p>See the project repository README for the complete set of straightforward data generation instructions.</p>"},{"location":"#example-query","title":"Example query","text":"<p>Here\u2019s an example query that counts the number of trips that start within 500 meters of each building:</p> <pre><code>SELECT\n    b.b_buildingkey,\n    b.b_name,\n    COUNT(*) AS nearby_pickup_count\nFROM trip t\nJOIN building b\nON ST_DWithin(t.t_pickup_loc, b.b_boundary, 500)\nGROUP BY b.b_buildingkey, b.b_name\nORDER BY nearby_pickup_count DESC;\n</code></pre> <p>This query performs a distance join, followed by an aggregation.  It\u2019s a great example of a query that\u2019s useful for performance benchmarking a spatial engine that can process vector geometries.</p>"},{"location":"#join-the-community","title":"Join the community","text":"<p>Feel free to start a GitHub Discussion or join the Discord community to ask the developers any questions you may have.</p> <p>We look forward to collaborating with you on these benchmarks!</p>"},{"location":"overview-methodology/","title":"SpatialBench Overview and Methodology","text":""},{"location":"overview-methodology/#spatialbench-overview-and-methodology","title":"SpatialBench Overview and Methodology","text":"<p>SpatialBench is an open benchmark suite of representative spatial queries designed to evaluate the performance of different engines at multiple scale factors.</p> <p>The SpatialBench queries are a great way to compare the relative performance between engines for analytical spatial workloads.  You can use a small scale factor for single-machine queries, and a large scale factor to benchmark an engine that distributes computations in the cloud.</p> <p>Let\u2019s take a deeper look at why SpatialBench is so essential.</p>"},{"location":"overview-methodology/#why-spatialbench","title":"Why SpatialBench?","text":"<p>Spatial workflows encompass queries such as spatial joins, spatial filtering, and spatial-specific operations, including KNN joins.</p> <p>General-purpose analytics query benchmarks don\u2019t cover spatial queries.  They focus on analytical queries, such as joins and aggregations, on tabular data. Here are some popular analytical benchmarks:</p> <ul> <li>TPC-H</li> <li>TPC-DS</li> <li>ClickBench</li> <li>YCSB</li> <li>db-benchmark</li> </ul> <p>The analytical benchmarks help analyze analytical performance, but that doesn\u2019t necessarily translate to spatial queries.  An engine can be blazing fast for a large tabular aggregation and terrible for spatial joins.</p> <p>SpatialBench is tailored for spatial queries.  It\u2019s the best modern option to assess the spatial performance of an engine.  Let\u2019s take a look at some of the older spatial benchmarks.</p>"},{"location":"overview-methodology/#hardware-and-software","title":"Hardware and software","text":"<p>SpatialBench runs benchmarks on commodity hardware, with software versions fully disclosed for each release.</p> <p>When comparing different runtimes, developers should make a good-faith effort to use similar hardware and software versions.  It\u2019s not helpful to compare one runtime with another runtime that has a lot less computational power.</p> <p>SpatialBench benchmarks should always be presented with associated hardware/software specifications so readers can assess the reliability of the comparison.</p>"},{"location":"overview-methodology/#accurately-comparing-different-engines","title":"Accurately comparing different engines","text":"<p>It is challenging to compare fundamentally different engines, such as PostGIS (an OLTP database), DuckDB (an OLAP database), and GeoPandas (a Python engine).</p> <p>For example, let\u2019s compare how two engines execute a query differently:</p> <ul> <li>PostGIS: create tables, load data into the tables, build an index (can be expensive), run the query</li> <li>GeoPandas: read data into memory and run a query</li> </ul> <p>PostGIS and GeoPandas execute queries differently, so you need to present the query runtime with caution.  For example, you can\u2019t just ignore the time it takes to build the PostGIS index because that can be the slowest part of the query.  That\u2019s a critical detail for users running ad hoc queries.</p> <p>The SpatialBench results strive to present runtimes for all relevant portions of the query so users are best informed about how to interpret the results.</p>"},{"location":"overview-methodology/#engine-tuning-in-benchmarks","title":"Engine tuning in benchmarks","text":"<p>Engines can be tuned by configuring settings or optimizing code.  For example, you can optimize Spark code by tuning the JVM.  You can optimize GeoPandas code by adding indexes.  Benchmarks that tune one engine and don\u2019t tune any of the other engines aren\u2019t reliable.</p> <p>All performance tuning is fully disclosed in the SpatialBench results.  Some results are presented both naively and fully tuned to give a better picture of out-of-the-box performance and what\u2019s possible for expert users.</p>"},{"location":"overview-methodology/#open-source-benchmarks-vs-vendor-benchmarks","title":"Open source benchmarks vs. vendor benchmarks","text":"<p>The SpatialBench benchmarks report results for some open source spatial engines/databases.</p> <p>The SpatialBench repository does not report results for any proprietary engines or vendor runtimes.  Vendors are free to use the SpatialBench data generators and run the benchmarks on their own.  We ask vendors to credit SpatialBench when they run the benchmarks and fully disclose the results so that other practitioners can reproduce the results.</p>"},{"location":"overview-methodology/#how-to-contribute","title":"How to contribute","text":"<p>There are a variety of ways to contribute to the SpatialBench project:</p> <ul> <li>Submit pull requests to add features</li> <li>Create issues for bug reports</li> <li>Reproduce results or help add new spatial engines</li> <li>Publish vendor benchmarks</li> </ul> <p>Here is how you can communicate with the team:</p> <ul> <li>Chat with us on the Apache Sedona Discord</li> <li>Create GitHub Discussions</li> </ul>"},{"location":"overview-methodology/#future-work","title":"Future work","text":"<p>In the next release, we will add raster datasets and raster queries.  These will stress test an engine\u2019s ability to analyze raster data.  They will also show performance when joining vector and raster datasets.</p>"}]}